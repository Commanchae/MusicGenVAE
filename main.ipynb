{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "import pypianoroll\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "midi_example = \"data/10,000_Maniacs/A_Campfire_Song.mid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions for Processing MIDI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tracks_with_most_notes(pretty_midi_file, top_n=4, ignore_instruments=[\"Drums\"]):\n",
    "  '''\n",
    "  Inputs:\n",
    "    pretty_midi_file (PrettyMIDI object)\n",
    "    top_n (int) - This tells the function how many instruments should be included. The result lists will be sorted from max note count to least notecount.\n",
    "\n",
    "  '''\n",
    "  instrument_notecount_list = []\n",
    "  # Loop through each instrument.\n",
    "  for instrument in pretty_midi_file.instruments:\n",
    "    if instrument.name in ignore_instruments:\n",
    "      continue\n",
    "    note_count = len(instrument.notes) # Cache the count of the notes of this instrument.\n",
    "    instrument_notecount_list.append([instrument, note_count])\n",
    "\n",
    "    # Sorts the list according to the note count.\n",
    "    instrument_notecount_list = sorted(instrument_notecount_list, key=lambda lst: lst[1], reverse=True)\n",
    "  instrument_list = [instrument_notecount[0] for instrument_notecount in instrument_notecount_list]\n",
    "\n",
    "  # If we do not have up to top_n tracks, let's just duplicate the last one.\n",
    "  top_instruments_list = instrument_list[0:top_n]\n",
    "  available_instrument_count = len(top_instruments_list)\n",
    "  if available_instrument_count < top_n:\n",
    "    top_instruments_list.extend([top_instruments_list[-1] * (top_n - available_instrument_count)])\n",
    "\n",
    "  return top_instruments_list\n",
    "\n",
    "\n",
    "def get_highest_notes(instrument_track):\n",
    "  highest_notes = {} # Highest note in each timestep.\n",
    "  for note in instrument_track.notes:\n",
    "    if note.start not in highest_notes.keys():\n",
    "      highest_notes[note.start] = [note, note.pitch]\n",
    "    else:\n",
    "      if note.pitch > highest_notes[note.start][1]: # Check if current note's pitch is higher than the cached pitch.\n",
    "        highest_notes[note.start] = [note, note.pitch]\n",
    "  return highest_notes\n",
    "\n",
    "\n",
    "def filter_for_highest_notes_only(instrument_track):\n",
    "  '''\n",
    "  Input(s):\n",
    "    instrument_track: PrettyMIDI Instrument object with notes in notes attribute.\n",
    "\n",
    "  Output(s):\n",
    "    instrument: PrettyMIDI instrument object with only the highest note at each timestep.\n",
    "  '''\n",
    "  highest_notes = get_highest_notes(instrument_track)\n",
    "\n",
    "  note_list = []\n",
    "  for key in sorted(highest_notes.keys()):\n",
    "    note_list.append(highest_notes[key][0])\n",
    "\n",
    "  instrument_info = {}\n",
    "  instrument_info[\"program\"] = instrument_track.program\n",
    "  instrument_info[\"is_drum\"] = instrument_track.is_drum\n",
    "  instrument_info[\"name\"] = instrument_track.name\n",
    "\n",
    "  instrument = pretty_midi.Instrument(instrument_info[\"program\"],\n",
    "                                      instrument_info[\"is_drum\"],\n",
    "                                      instrument_info[\"name\"],\n",
    "                                      )\n",
    "\n",
    "  instrument.notes = note_list\n",
    "  return instrument\n",
    "\n",
    "\n",
    "def convert_rolls_to_multitrack(piano_rolls, instrument_rolls, output_filepath):\n",
    "    '''\n",
    "    Inputs:\n",
    "        piano_rolls: np array shape (4, 800, 128) *One instance of data*\n",
    "        instrument_rolls: np array shape (4, 128) *One instance of data*\n",
    "        output_filepath: str, name/path of file to output.\n",
    "    '''\n",
    "    # First, we create track objects.\n",
    "    track_list = []\n",
    "    for i in range(instrument_rolls.shape[0]):\n",
    "        track = pypianoroll.BinaryTrack()\n",
    "\n",
    "        program = np.argmax(instrument_rolls[i]) + 1 # Adds one so it corresponds to the correct MIDI instrument.\n",
    "\n",
    "        track.program = program\n",
    "        track.is_drum = False\n",
    "        track.pianoroll = piano_rolls[i]\n",
    "\n",
    "        track_list.append(track)\n",
    "\n",
    "    multitrack = pypianoroll.Multitrack(resolution=4, tracks=track_list)\n",
    "    pypianoroll.write(output_filepath, multitrack)\n",
    "    return multitrack\n",
    "\n",
    "\n",
    "def get_onehotencoding_instrument(track):\n",
    "  max_instruments = 128 # There are 128 possible instruments according to https://fmslogo.sourceforge.io/manual/midi-instrument.html\n",
    "  program = track.program - 1\n",
    "  return np.eye(max_instruments)[program]\n",
    "\n",
    "def convert_multitrack_to_rolls(multitrack):\n",
    "  # Convert multitrack to piano rolls of shape (4, t, 128), where t is the number of timesteps.\n",
    "  # and to instrument rolls of shape (4, 128).\n",
    "  pianoroll_list = []\n",
    "  instrumentroll_list = []\n",
    "  for instrument in multitrack.tracks:\n",
    "    pianoroll = np.expand_dims(instrument.pianoroll, axis=0) # Expand dims because initially they're (t, 128). We want them as (1, t, 128) so we can stack along the 0th axis.\n",
    "    pianoroll_list.append(pianoroll)\n",
    "\n",
    "    # Creates a onehotencoding for the instrument.\n",
    "    instrumentroll = get_onehotencoding_instrument(instrument)\n",
    "    instrumentroll_list.append(instrumentroll)\n",
    "\n",
    "  pianoroll_arr = np.vstack(pianoroll_list)\n",
    "  instrumentroll_arr = np.vstack(instrumentroll_list)\n",
    "\n",
    "  return pianoroll_arr, instrumentroll_arr\n",
    "\n",
    "def parse_rolls_from_midi_filepath(filepath):\n",
    "  '''\n",
    "  Input:\n",
    "    filepath: The relative/absolute filepath to the Midi file.\n",
    "  Outputs:\n",
    "    piano_roll: The piano roll of the top 4 instruments of the track. Shape (4, t, 128). Where t is the number of timesteps.\n",
    "      Note: 128 is the number of possible pitches.\n",
    "    instrument_roll: The instrument rolls of the top 4 instruments of the track. Shape (4, 128).\n",
    "      Note: 128 is different from above, here it is the number of possible instruments in MIDI.\n",
    "\n",
    "  '''\n",
    "  try:\n",
    "    midi_file = pretty_midi.PrettyMIDI(filepath)\n",
    "    midi_file.instruments = get_tracks_with_most_notes(midi_file) # Gets the top 4 (default) instruments with most notes.\n",
    "\n",
    "    # Gets the highest note in each instrument (only 1 note per instrument per timestep).\n",
    "    top_instruments = [filter_for_highest_notes_only(instrument) for instrument in midi_file.instruments]\n",
    "    midi_file.instruments = top_instruments # Sets the instruments attribute to the top instruments.\n",
    "\n",
    "    # Convert from PrettyMIDI object to pypianoroll Multitrack object to easily get the piano roll.\n",
    "    multitrack = pypianoroll.from_pretty_midi(midi_file, resolution = 6)\n",
    "\n",
    "    piano_roll, instrument_roll = convert_multitrack_to_rolls(multitrack)\n",
    "  except:\n",
    "    print(\"Error: \", filepath)\n",
    "    piano_roll, instrument_roll = None, None\n",
    "  return piano_roll, instrument_roll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "data_size = 1000 # Hardcoded to only train the model with 1000 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self):\n",
    "    self.data_x = np.load(\"hooktheory-npy-data/data_x_augmented.npy\").astype(np.float32)\n",
    "    self.prev_x = np.load(\"hooktheory-npy-data/prev_x_augmented.npy\").astype(np.float32)\n",
    "    pass\n",
    "\n",
    "  def __len__(self):\n",
    "    return data_size # Amount of data we have.\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.prev_x[idx], self.data_x[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset()\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Variational Autoencoder via PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "EPOCHS = 20\n",
    "latent_size = 8\n",
    "data_size = 1000\n",
    "kld_factor = 0.001\n",
    "kld_mod = -1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HT_VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HT_VAE, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_size = latent_size\n",
    "        # Encoding #\n",
    "        self.enc_fc1 = nn.Linear(2048, 1024)\n",
    "        self.enc_bn1 = nn.BatchNorm1d(1024)\n",
    "        self.enc_fc2 = nn.Linear(1024, 512)\n",
    "        self.enc_bn2 = nn.BatchNorm1d(512)\n",
    "        self.enc_fc3 = nn.Linear(512, 128)\n",
    "        self.enc_bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.enc_mu = nn.Linear(128, self.latent_size)\n",
    "        self.enc_logvar = nn.Linear(128, self.latent_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Decoding #\n",
    "        self.dec_fc1 = nn.Linear(self.latent_size, 128)\n",
    "        self.dec_bn1 = nn.BatchNorm1d(128)\n",
    "        self.dec_fc2 = nn.Linear(128, 512)\n",
    "        self.dec_bn2 = nn.BatchNorm1d(512)\n",
    "        self.dec_fc3 = nn.Linear(512, 2048)\n",
    "        self.dec_bn3 = nn.BatchNorm1d(2048)\n",
    "\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        #self.batchnorm_1 = nn.Batchnorm1d()\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.constant_(layer.bias, 0.0)\n",
    "            elif isinstance(layer, nn.GRU):\n",
    "                for name, param in layer.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        nn.init.xavier_uniform_(param)\n",
    "                    elif 'bias' in name:\n",
    "                        nn.init.constant_(param, 0.0)\n",
    "                        \n",
    "    def encode(self, prev_x):\n",
    "        # Prev_X is of shape [batch_size, 16, 128]\n",
    "        data = torch.flatten(prev_x, start_dim=1) # [batch_size, 2048]\n",
    "\n",
    "        # Go through FC with 512 neurons, followed by RELU\n",
    "        h0 = self.enc_fc1(data) # from [b, 2048] to [b, 1024]\n",
    "        h0 = self.enc_bn1(h0)\n",
    "        h0 = self.tanh(h0)\n",
    "\n",
    "        h1 = self.enc_fc2(h0) # from [b, 1024] to [b, 512]\n",
    "        h1 = self.enc_bn2(h1)\n",
    "        h1 = self.tanh(h1)\n",
    "        h1 = self.dropout(h1)\n",
    "\n",
    "        h2 = self.enc_fc3(h1)\n",
    "        h2 = self.enc_bn3(h2)\n",
    "        h2 = self.tanh(h2)\n",
    "\n",
    "\n",
    "        # Parallel mu and logvar generation, followed by RELU.\n",
    "        mu = self.enc_mu(h2) # from [b, 128] to [b, 16]\n",
    "        mu = self.relu(mu)\n",
    "\n",
    "        log_var = self.enc_logvar(h2) # from [b, 128] to [b, 16]\n",
    "        log_var = self.relu(log_var)\n",
    "        log_var = log_var + 1\n",
    "\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)  # Standard deviation\n",
    "        eps = torch.randn_like(std)     # Sample epsilon from standard normal distribution\n",
    "        z = mu + eps * std              # Reparameterization trick\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        h0 = self.dec_fc1(z)\n",
    "        h0 = self.dec_bn1(h0)\n",
    "        h0 = self.relu(h0)\n",
    "        h0 = self.dropout(h0)\n",
    "\n",
    "        h1 = self.dec_fc2(h0)\n",
    "        h1 = self.dec_bn2(h1)\n",
    "        h1 = self.relu(h1)\n",
    "\n",
    "        h2 = self.dec_fc3(h1)\n",
    "        h2 = h2.view(self.batch_size, 16, 128)\n",
    "        output = self.sigmoid(h2)\n",
    "        return output\n",
    "\n",
    "    def forward(self, prev_x):\n",
    "        mu, log_var = self.encode(prev_x)\n",
    "\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "\n",
    "        generated_bar = self.decode(z)\n",
    "        return generated_bar, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HT_VAE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_gaussian_kl_loss(mu, log_var):\n",
    "  KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=1)\n",
    "  return KLD.mean()\n",
    "\n",
    "def reconstruction_loss(x_reconstructed, x):\n",
    "  bce_loss = nn.BCELoss()\n",
    "  return bce_loss(x_reconstructed, x)\n",
    "\n",
    "def vae_loss(y_pred, y_true, epoch):\n",
    "  mu, log_var, reconstructed_x = y_pred\n",
    "  reconstructed_loss = reconstruction_loss(reconstructed_x, y_true)\n",
    "  kld_loss = vae_gaussian_kl_loss(mu, log_var)\n",
    "  #print(\"RECON:\", reconstructed_loss, \" KLD: \", kld_loss)\n",
    "\n",
    "  return reconstructed_loss + kld_factor*kld_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "training_loader = train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "  running_loss = 0.\n",
    "  for i, data in enumerate(training_loader):\n",
    "    prev_x, current_x = data\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    generated_bar, mu, log_var = model(prev_x)\n",
    "\n",
    "    loss = vae_loss((mu, log_var, generated_bar), current_x, epoch_index)\n",
    "    print(f\"Epoch {epoch_index} Loss {i}: {loss.item()}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  model.train(True)\n",
    "  train_one_epoch(epoch, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_output_to_pr(output, mean_probability=False, input_probability=0.1, tensor_output=False):\n",
    "    '''\n",
    "    Input: Tensor output of shape [instrument_size, timestep, pitch_range]\n",
    "    '''\n",
    "\n",
    "    # For each instrument size, we want to compile onehot encodings of shape timestep, pitch_range.\n",
    "    pr_output = None\n",
    "    track_outputs = []\n",
    "    for track in output:\n",
    "        track_output = None\n",
    "        for idx, val in zip(track.max(dim=1).indices, track.max(dim=1).values):\n",
    "            pitch = idx.item()\n",
    "\n",
    "            onehot = np.eye(128)[pitch]\n",
    "            if track_output is None:\n",
    "                track_output = np.array(onehot)\n",
    "            else:\n",
    "                track_output = np.vstack((track_output, onehot))\n",
    "\n",
    "        track_outputs.append(track_output)\n",
    "    pr_output = np.stack(track_outputs)\n",
    "    pr_output = np.array(pr_output)\n",
    "    pr_output = pr_output.astype(int)\n",
    "    \n",
    "    return pr_output if not tensor_output else torch.from_numpy(pr_output)\n",
    "            \n",
    "def save_output_to_mid(output, filename, instrument=np.eye(128)[33].astype(int)[None, :]):\n",
    "    pr = convert_output_to_pr(output, mean_probability=True)\n",
    "    convert_rolls_to_multitrack(pr, instrument , filename)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
